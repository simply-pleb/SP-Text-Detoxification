{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from transformers import BartTokenizerFast, DataCollatorWithPadding\n",
    "from transformers import BartModel, BartForConditionalGeneration, Trainer, TrainingArguments, EvalPrediction\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = 'facebook/bart-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87102/3124605388.py:1: DtypeWarning: Columns (3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"../data/intermit/merged_dataset.tsv\", sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/intermit/merged_dataset.tsv\", sep='\\t')\n",
    "data = data.iloc[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizerFast.from_pretrained(CHECKPOINT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized = tokenizer(list(data[\"toxic\"].values), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# type(data[\"toxic\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   700,    56,  ...,     1,     1,     1],\n",
       "        [    0,   417,  6343,  ...,     1,     1,     1],\n",
       "        [    0,   757,    45,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0, 40992,   162,  ...,     1,     1,     1],\n",
       "        [    0, 17762,    64,  ...,     1,     1,     1],\n",
       "        [    0,   100,   818,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tokenized = tokenizer(list(data[\"neutral1\"].values), padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,  700,   21,  ...,    1,    1,    1],\n",
       "         [   0,  243,   74,  ...,    1,    1,    1],\n",
       "         [   0,  100,  437,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [   0, 8267,  127,  ...,    1,    1,    1],\n",
       "         [   0,  627,  604,  ...,    1,    1,    1],\n",
       "         [   0,  100,  818,  ...,    1,    1,    1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenized.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"input_ids\": X_tokenized['input_ids'], \"attention_mask\": X_tokenized['attention_mask'], \"label_ids\": y_tokenized['input_ids']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'label_ids'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split your dataset into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data = train_test_split(train_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict(train_data)\n",
    "test_data = Dataset.from_dict(test_data)\n",
    "validation_data = Dataset.from_dict(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = DatasetDict({\"train\":train_data, \"test\":test_data, \"validation\":validation_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'label_ids'],\n",
       "        num_rows: 14400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'label_ids'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'label_ids'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efc391f9a0743e69b492e702cc84fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214ffa3a0a1b4678a74ce00dd66a1bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0cd15ac8a84515b1a7f061802ce3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datadict.save_to_disk(dataset_dict_path=\"../data/intermit/dataset-dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    datadict[\"train\"], shuffle=True, batch_size=16, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    datadict[\"validation\"], batch_size=16, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([16, 170]),\n",
       " 'attention_mask': torch.Size([16, 170]),\n",
       " 'labels': torch.Size([16, 155])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizerFast.from_pretrained(CHECKPOINT)\n",
    "model = BartForConditionalGeneration.from_pretrained(CHECKPOINT)\n",
    "\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9fe39c84b541f5bd7e620af3b6fbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"glue\", \"stsb\")\n",
    "model.eval()\n",
    "\n",
    "predictions_list = []\n",
    "references_list = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Flatten the predictions and references tensors\n",
    "    flat_predictions = predictions.view(-1)\n",
    "    flat_references = batch[\"labels\"].view(-1)\n",
    "\n",
    "    predictions_list.append(flat_predictions)\n",
    "    references_list.append(flat_references)\n",
    "\n",
    "\n",
    "# Concatenate the lists of predictions and references into tensors\n",
    "predictions = torch.cat(predictions_list, dim=0)\n",
    "references = torch.cat(references_list, dim=0)\n",
    "\n",
    "# Ensure that predictions and references are on the same device\n",
    "predictions = predictions.to(device)\n",
    "references = references.to(device)\n",
    "\n",
    "# Compute the metric\n",
    "metric.add_batch(predictions=predictions, references=references)\n",
    "result = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pearson': 0.7458301157089653, 'spearmanr': 0.999611734921778}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 12375,    64,  ...,     1,     1,     1],\n",
      "        [    0,  1185,   214,  ...,     1,     1,     1],\n",
      "        [    0,  6460,    10,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   506, 24029,  ...,     1,     1,     1],\n",
      "        [    0,  2847,    52,  ...,     1,     1,     1],\n",
      "        [    0, 27037, 38538,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    0,  8155,    64,  ...,     1,     1,     1],\n",
      "        [    0, 13724,     6,  ...,     1,     1,     1],\n",
      "        [    0,  6968,   197,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   100,   269,  ...,     1,     1,     1],\n",
      "        [    0,  2527,    52,  ...,     1,     1,     1],\n",
      "        [    0, 27037, 38538,  ...,     1,     1,     1]])}\n"
     ]
    }
   ],
   "source": [
    "qq = eval_dataloader.__iter__()\n",
    "print(qq.__next__())\n",
    "batch = qq.__next__()\n",
    "batch = {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yy7/miniconda3/envs/dl-python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0,   118,    21,    95, 27537,    19, 23644,   479,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0, 16625,  1623,     6,    84,  8453,     6,   362,   130,\n",
       "          6317,     9,    84,   275,    13,    10,  1402,   744,     4,     2],\n",
       "        [    2,     0,  2527,    38,   236,    47,     7,   356,    23,   127,\n",
       "           652,    13,    10,    94,    86,     4,     2,     1,     1,     1],\n",
       "        [    2,     0,   102,   221,     4,   100,     4, 34850,  1295,   272,\n",
       "         13093,   102,  1516,   848,   734,     8,   172,   272, 13093,     2],\n",
       "        [    2,     0,  1185,   357,  1407,   162,   124,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0,  6968,   236,     7,   283,     7,   127,  6085,   116,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0,   100,  2638,   167, 30576,   514, 22475,     4,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0,   102,  1459,   946,    15,  2156,  4016,   179,    24,\n",
       "           479,     2,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0, 10010,   686,   109,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0,  1250,     5, 18282,    32,  1462,   122,  2156,    98,\n",
       "           939,  1034,    14,    21,    49,  1445,   301,   128,    29,     2],\n",
       "        [    2,     0,  6968,   214,     5,  1802,     9,     5, 15939,   990,\n",
       "          1900,     4,     2,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0,  9178, 13531,    57, 17487,   939,   192,    47,    57,\n",
       "          3545,   179,   103, 21504,  2682, 12056, 29784,   479,     2,     1],\n",
       "        [    2,     0,   405,    18,    57,    10,  5373,   367,   688,     6,\n",
       "          2282,    75,    24,   116,     2,     1,     1,     1,     1,     1],\n",
       "        [    2,     0, 12196,    16,  1593,    19,    47,   116,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0, 37468,     6,   120,     5,  7105,    66,     9,   259,\n",
       "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0, 12229,     6,    47,  1747,    33,    14,  3553,   241,\n",
       "         38297,    47,   236,    19,   110,  4486,  6096,     8,    69,     2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was just fuckin wit cha .\n",
      "i was just kidding with cha .\n",
      "Your husband, our king, has taken 300 of our finest to slaughter.\n",
      "your husband, our king, took three hundred of our best for a certain death.\n",
      "So I want you to look at my face one last fucking time.\n",
      "so I want you to look at my face for a last time.\n",
      "A P.I. Investigating gedda gets killed... then gedda gets killed... maybe this dirty copdid them both.\n",
      "a P.I. Investigating Gedda gets killed... and then Gedd\n",
      "u better follow me back for i swag ya ass and not in a good way\n",
      "You better follow me back\n",
      "Do you want to come in my mouth?\n",
      "you want to come to my mouth?\n",
      "fucking loved those grenade water balloons .\n",
      "I loved those grenade water balloons.\n",
      "aite hold on , downloadin shit .\n",
      "aite hold on , downloadin it .\n",
      "they sure as hell fucking do .\n",
      "they sure do\n",
      "all the gunmen are dead now , so i hope that was their entire fucking life 's goal .\n",
      "all the gunmen are dead now , so i hope that was their entire life 's\n",
      "You're the beneficiary of the cruelest kind of murder.\n",
      "you're the victim of the cruelest murder.\n",
      "how ya been ? i see you been tweetin some inspirational shit lately lol .\n",
      "how ya been ? i see you been tweetin some inspirational stuff lately lol .\n",
      "This has been a ridiculous few weeks, hasn't it?\n",
      "it's been a crazy few weeks, hasn't it?\n",
      "what is wrong with you leftist nutbars .\n",
      "what is wrong with you?\n",
      "Kyle, go pack our shit, now.\n",
      "Kyle, get the hell out of here.\n",
      "Hey, you could've finally had that threesome you want with your fake girlfriend and her roommate.\n",
      "hey, you finally have that threesome you want with your fake girlfriend and her\n"
     ]
    }
   ],
   "source": [
    "# logits = outputs.logits[0].tolist()\n",
    "# # Convert logits to text\n",
    "# generated_text = tokenizer.decode(logits, skip_special_tokens=True)\n",
    "\n",
    "# # Print the generated text\n",
    "# print(generated_text)\n",
    "inputs = batch['input_ids']\n",
    "for i in range(len(inputs)):\n",
    "    print(tokenizer.decode(inputs[i], skip_special_tokens=True, clean_up_tokenization_spaces=False))\n",
    "    print(tokenizer.decode(outputs[i], skip_special_tokens=True, clean_up_tokenization_spaces=False))\n",
    "# print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in inputs])\n",
    "# print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/bart-paradetox\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
